{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c55a2-9949-4c7d-a9e2-0797ac2f3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch_geometric\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU\n",
    "from Networks import *\n",
    "from Data_Functions import *\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from Utils import *\n",
    "from Subgrid_Funcs import *\n",
    "import torch.distributed as dist\n",
    "\n",
    "from Parallel import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a8f2d5-020c-43aa-a273-a0f72c1eb9da",
   "metadata": {},
   "source": [
    "Here is a set of functions and use case snippets to help give an overview of where we would most need guidence in optimizing our code. The biggest need at the moment is in managing data throughout the training process, particularily as we aim to scale up the quantity of avalible training data.\n",
    "\n",
    "I think there are a few primary areas where we become bottlenecked with data:\n",
    "\n",
    "1. Loading data onto RAM prior to training. Since the data that we have is large and stored in Xarray, the current process involves loading the dataset full dataset onto RAM prior to training. This is already a large memory cost, particularly when we train across multiple recurrent passes (further bottlneck details below). It would be helpful to get advice on addressing this memory cost, or better practices with how to train from data on disk (particularly how to strike a balance between storing a copy of highly preprocessed data and efficiency)\n",
    "2. When constructing the dataset, we build copies of the data for each recurrent pass, i.e. the inputs for a given step, n, would be $\\Phi(t=n\\Delta t:(N_{samples}+n)\\Delta t)$. It would probably be more memory efficient to store a single copy of values from $0:(N_{samples}+N_{steps})\\Delta t)$, where $N_{samples}$ is the total number of recurrent passes in training and $N_{steps}$ is the number of reccurent passes during training. I imagine that using pointers in place of copies would be better, but I am not sure quite how to go about that.\n",
    "3. At the moment, when using multiple GPUs, I load a chunk of the data set onto each GPU. We use staggered data in each chunk so that each GPU has access to data throughout the dataset. This is to avoid the bottleneck of having to load all of the data linearly and speeds up the code significantly. Not sure if this is best practice or if the adjusments to other points may allow for a better split across GPUs.\n",
    "4. We are moving to synthesise data across several sources. For this we want to both be able to train simultaneously, but it is not exactly clear the best way to synthesize this given the previous constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71586a7-e7c3-4d53-9a42-0b42365c9ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading data from zarr stores\n",
    "\n",
    "# input_vars - variables given as input to the network (usually the same as the output)\n",
    "# extra_vars - aditional boundary or residual values given to the network (these are replaced with ground truth during rollout)\n",
    "# output_vars - variables predicted by the network (usually the input at a future time step)\n",
    "# lag - number of time steps for predictions (our time step is 1 day)\n",
    "# lat - latitude bounds for the region to consider\n",
    "# lon - longitude bounds for the region to consider\n",
    "# Nb - the number of lateral boundary points for the data\n",
    "# run_type = forcing condition to use (can be \"\" for preindustrial control or \"2x\" for CO2 doubling experiment) \n",
    "\n",
    "def gen_data_025_lateral(input_vars,extra_vars,output_vars,lag,lat,lon,Nb=2,run_type=\"\"):\n",
    "    var_dict = {\"um\":\"u_mean\",\"vm\":\"v_mean\",\"Tm\":\"T_mean\",\n",
    "                \"ur\":\"u_res\",\"vr\":\"v_res\",\"Tr\":\"T_res\",\n",
    "               \"u\":\"u\",\"v\":\"v\",\"T\":\"T\",\n",
    "               \"tau_u\":\"tau_u\",\"tau_v\":\"tau_v\",\"tau\":\"tau\",\n",
    "               \"t_ref\":\"t_ref\"}\n",
    "    \n",
    "    \n",
    "    if run_type != \"\":\n",
    "        run_type = \"_\" + run_type\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "        data = xr.open_zarr(\"/scratch/as15415/Data/Emulation_Data/Global_Ocean_025deg\"+run_type+\".zarr\").sortby(\"time\")\n",
    "        try:\n",
    "            data_atmos = xr.open_zarr(\"/scratch/as15415/Data/Emulation_Data/Data_Atmos_025_deg\"+run_type+\".zarr\").drop([\"xu_ocean\",\"T_mean\"]).assign_coords({\"lon\":data.xu_ocean.data}).sortby(\"time\")\n",
    "        except:\n",
    "            data_atmos = xr.open_zarr(\"/scratch/as15415/Data/Emulation_Data/Data_Atmos_025_deg\"+run_type+\".zarr\").sortby(\"time\")    \n",
    "        data_atmos = data_atmos.rename_dims({\"lat\":\"yu_ocean\",\"lon\":\"xu_ocean\"})\n",
    "        data_atmos = data_atmos.rename({\"lat\":\"yu_ocean\",\"lon\":\"xu_ocean\"})\n",
    "    \n",
    "    \n",
    "        data = data.sel(time=slice(data_atmos.time[0],data_atmos.time[-1]))\n",
    "        data_atmos = data_atmos.sel(time=slice(data.time[0],data.time[-1]))\n",
    "    \n",
    "    data = xr.merge([data,data_atmos])\n",
    "    \n",
    "    data = data.sel(yu_ocean=slice(lat[0],lat[1]),xu_ocean=slice(lon[0],lon[1]))\n",
    "    \n",
    "    inputs = []\n",
    "    extra_in = []\n",
    "    outputs = []\n",
    "    \n",
    "    for var in input_vars:\n",
    "        inputs.append(data[var_dict[var]])\n",
    "\n",
    "    for var in extra_vars:\n",
    "        extra_in.append(data[var_dict[var]])\n",
    "\n",
    "    for var in input_vars:             \n",
    "        temp = data[var_dict[var]].copy(deep=True)\n",
    "        temp[:,Nb:-Nb,Nb:-Nb]  = 0.0 *temp[0,Nb:-Nb,Nb:-Nb]\n",
    "        extra_in.append(temp)\n",
    "        \n",
    "    for var in output_vars:\n",
    "        outputs.append(data[var_dict[var]][lag:])\n",
    "        \n",
    "    inputs = tuple(inputs)\n",
    "    extra_in = tuple(extra_in)\n",
    "    outputs = tuple(outputs)\n",
    "\n",
    "    return inputs, extra_in, outputs\n",
    "\n",
    "\n",
    "# function for generating a set of lagged inputs\n",
    "# s - starting index that the dataset is indexing from\n",
    "# e - ending index that the dataset is indexing from (note, the data may go past this index\n",
    "#    when including outputs or future reccurent passes)\n",
    "# interval - subsampling between training snapshots\n",
    "# lag - number of time steps for predictions (our time step is 1 day)\n",
    "# hist - number of previous time steps included as an input the network input (currently only use hist = 1)\n",
    "# inputs - array of xarray DataArrays for each input variable as inputs are defined above\n",
    "# extra_in - array of xarray DataArrays for each boundary or residual variable given to the network (these are replaced with ground truth during rollout)\n",
    "\n",
    "\n",
    "\n",
    "def gen_data_in(step,s,e,interval,lag,hist,inputs,extra_in):\n",
    "    s = s+lag*step\n",
    "    e = e+lag*step    \n",
    "    num_outs = len(inputs)\n",
    "    num_extra = len(extra_in)\n",
    "    temp_inputs = []\n",
    "    for j in range(num_outs):\n",
    "        temp_inputs.append(inputs[j][s:e:interval].to_numpy())\n",
    "    temp_extra = []\n",
    "    for j in range(num_extra):\n",
    "        temp_extra.append(extra_in[j][s:e:interval].to_numpy())\n",
    "        \n",
    "    data_in = np.stack((*temp_inputs,\n",
    "                         *temp_extra),-1)\n",
    "    \n",
    "    for i in range(hist):\n",
    "        temp_inputs = []\n",
    "        for j in range(num_outs):\n",
    "            temp_inputs.append(np.expand_dims(inputs[j][s-(hist-i)*lag:e-(hist-i)*lag:(interval)].to_numpy(),-1))\n",
    "        data_in = np.concatenate((data_in,*temp_inputs),axis=3)\n",
    "    return data_in\n",
    "\n",
    "\n",
    "# function for generating a corresponding set of lagged inputs\n",
    "# s - starting index that the dataset is indexing from\n",
    "# e - ending index that the dataset is indexing from (note, the data may go past this index\n",
    "#    when including outputs or future reccurent passes)\n",
    "# interval - subsampling between training snapshots\n",
    "# inputs - array of xarray DataArrays for each output variable as outputs are defined above\n",
    "\n",
    "def gen_data_out(step,s,e,lag,interval,outputs):\n",
    "    s = s+lag*step\n",
    "    e = e+lag*step\n",
    "    \n",
    "    num_outs = len(outputs)\n",
    "    temp_outputs = []\n",
    "    for j in range(num_outs):\n",
    "        temp_outputs.append(outputs[j][s:e:interval].to_numpy())    \n",
    "    \n",
    "    data_out = np.stack(temp_outputs,-1)\n",
    "    return data_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ea3e8-ea6d-479f-b3a7-461692fb7464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code for loading a dataset for a region of the globe\n",
    "\n",
    "\n",
    "#select the inputs and additional boundary information\n",
    "\n",
    "# choose u,v,T\n",
    "exp_num_in = \"3\"\n",
    "# choose tau_u, tau_v, T_atm\n",
    "exp_num_extra = \"12\"\n",
    "# choose u,v,T\n",
    "exp_num_out = \"2\"\n",
    "\n",
    "#Choose region for training\n",
    "region = \"Gulf_Stream_Ext\"  \n",
    "\n",
    "#Choose subsampling interval (we use 1 in all our cases)\n",
    "interval = 1\n",
    "\n",
    "#Choose number of samples for training and validation\n",
    "N_samples = 4000\n",
    "N_val = 300\n",
    "\n",
    "\n",
    "# choose the number of previous steps included in the input (we use hist = 0, this is from previous exploration\n",
    "# and kept for potential future exploration)\n",
    "hist = 0\n",
    "# lag - number of time steps between predictions (our time step is 1 day)\n",
    "lag = 1\n",
    "# set number of recurrent passes used during training\n",
    "steps = 4\n",
    "\n",
    "# set number of boundary points given during training\n",
    "Nb = 4\n",
    "\n",
    "# get domain boundaries for the region\n",
    "if region == \"Gulf_Stream_Ext\"\n",
    "    lat = [27, 50]\n",
    "    lon = [-82,-35]      \n",
    "    \n",
    "    \n",
    "s_train = lag*hist\n",
    "e_train = s_train + N_samples*interval\n",
    "e_test = e_train + interval*N_val\n",
    "\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "inpt_dict = {\"1\":[\"um\",\"vm\"],\"2\":[\"um\",\"vm\",\"ur\",\"vr\"],\"3\":[\"um\",\"vm\",\"Tm\"],\n",
    "            \"4\":[\"um\",\"vm\",\"ur\",\"vr\",\"Tm\",\"Tr\"],\"5\":[\"ur\",\"vr\"],\"6\":[\"ur\",\"vr\",\"Tr\"],\n",
    "            \"7\":[\"Tm\"],\"8\":[\"Tm\",\"Tr\"],\"9\":[\"u\",\"v\"],\"10\":[\"u\",\"v\",\"T\"],\n",
    "            \"11\":[\"tau_u\",\"tau_v\"],\"12\":[\"tau_u\",\"tau_v\",\"t_ref\"]} \n",
    "extra_dict = {\"1\":[\"ur\",\"vr\"],\"2\":[\"ur\",\"vr\",\"Tm\"],\n",
    "            \"3\":[\"Tm\"],\"4\":[\"ur\",\"vr\",\"Tm\",\"Tr\"],\"5\":[],\"6\":[\"um\",\"vm\"],\n",
    "             \"7\":[\"um\",\"vm\",\"Tm\"], \"8\": [\"um\",\"vm\",\"Tm\",\"Tr\"],\n",
    "              \"9\":[\"ur\",\"vr\",\"tau_u\",\"tau_v\"],\"10\":[\"tau_u\",\"tau_v\"],\n",
    "              \"11\":[\"t_ref\"],\"12\":[\"tau_u\",\"tau_v\",\"t_ref\"],\n",
    "             \"13\":[\"ur\",\"vr\",\"Tr\",\"tau_u\",\"tau_v\",\"t_ref\"]} \n",
    "out_dict = {\"1\":[\"um\",\"vm\"],\"2\":[\"um\",\"vm\",\"Tm\"],\"3\":[\"ur\",\"vr\"],\n",
    "           \"4\":[\"ur\",\"vr\",\"Tr\"],\"5\":[\"u\",\"v\"],\"6\":[\"u\",\"v\",\"T\"]}\n",
    "\n",
    "grids = xr.open_dataset('/scratch/zanna/data/CM2_grids/Grid_cm25_Vertices.nc')\n",
    "grids = grids.sel({\"yu_ocean\":slice(lat[0],lat[1]),\"xu_ocean\":slice(lon[0],lon[1])})\n",
    "\n",
    "area = torch.from_numpy(grids[\"area_C\"].to_numpy()).to(device=device)\n",
    "\n",
    "inputs = inpt_dict[exp_num_in]\n",
    "extra_in = extra_dict[exp_num_extra]\n",
    "outputs = out_dict[exp_num_out]\n",
    "\n",
    "str_in = \"\".join([i + \"_\" for i in inputs])\n",
    "str_ext = \"\".join([i + \"_\" for i in extra_in])\n",
    "str_out = \"\".join([i + \"_\" for i in outputs])\n",
    "\n",
    "print(\"inputs: \" + str_in)\n",
    "print(\"extra inputs: \" + str_ext)\n",
    "print(\"outputs: \" + str_out)\n",
    "\n",
    "N_atm = len(extra_in)\n",
    "N_in = len(inputs)\n",
    "N_extra = N_atm + N_in\n",
    "N_out = len(outputs)\n",
    "\n",
    "num_in = int((hist+1)*N_in + N_extra)\n",
    "\n",
    "\n",
    "inputs, extra_in, outputs = gen_data_025_lateral(inputs,extra_in,outputs,lag,lat,lon,Nb)\n",
    "\n",
    "wet = xr.zeros_like(inputs[0][0])\n",
    "for data in inputs:\n",
    "    wet +=np.isnan(data[0])\n",
    "wet = np.isnan(xr.where(wet==0,np.nan,0))\n",
    "wet = np.nan_to_num(wet.to_numpy())\n",
    "wet = torch.from_numpy(wet).type(torch.float32).to(device=\"cpu\")\n",
    "\n",
    "data_in_train = []\n",
    "data_out_train = []\n",
    "for i in range(steps):\n",
    "    data_in_train.append(gen_data_in(i,s_train,e_train,\n",
    "                                     interval,lag,\n",
    "                                     hist,inputs,extra_in))\n",
    "    data_out_train.append(gen_data_out(i,s_train,e_train,\n",
    "                                       lag,interval,\n",
    "                                       outputs))\n",
    "\n",
    "train_data = data_CNN_steps_Lateral(data_in_train,data_out_train,\n",
    "                                        steps,wet,N_atm,Nb,device=device)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_new",
   "language": "python",
   "name": "torch_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
